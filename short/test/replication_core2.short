(*
  - FIX: now I should be consuming rdfailed, and rdack from all replica, and
         not turn around the send release until I've heard from all the replica
         Version broken since Any(rdack) differs in value since old rdack wasnt
         consumed when it should have been.
  - FIX: don't send release to all replica, only the ones that client locked
         * this is a problem, it means I need to know which replica has acked
           a rd in the handler for rdfailed. Currently a reference to 
           SENDERS(rdack) does not rewrite and generate the needed inbox scans.
           (need to do the same thing as if rdfailed were a multi message hndl)
  - FIX: had to remove the broken notion of selective receive and revert
         to reading head positions of the inbox only. See how far we can get.
  - FIX: turned crash handling back on. Now the Consistency statement needs to
         take into account that crash replicas are are not part of the mix.
  - FIX: now I'm seeing an interesting counter example that involves a crashed
         replica. Client X hasn't heard the crash of replica A yet and has
         a very old rdack (data=FALSE) in the inbox. The once the 2nd rd msg
         from X to B finally reaches (after Y has done full rd/wr cycle and
         updated the data to TRUE) B, B replies with a rdack(data=TRUE), which
         is trapped by ANY(.) stating the two rdacks we're using to flip the
         data value between rd and wr aren't equal (indeed data is TRUE/FALSE).
         * At first one might thing the problem is that selective receive 
           would deliver the crash to X, BUT the problem is really that A's
           crash message hasn't been put on the wire at all
         * What I think is needed is that Y infects B with the reduced view
           such that by the time the rd from X hits B, it will carry back the
           changed view and we won't await a result from A (thus the ANY 
           won't be an issue).
         * But first, I'll separate Client from Replica crash handling. The
           handling of client crashes should be easy to validate.
         * Cool - Client crashing is handled correctly.
  - FIX: added the ability to thread view state on writes to replica and
         rdacks subsequently carry them to clients
         * added ANY msg handler to fish out rdack that carries a view change
          (rview # view(Replica) /\ rview # NIL)
         * still, the any2 assertion (above) hits since the ANY handler is
           not invoked as the Clients inbox is clogged by a rdack (the first)
           that has rview=NIL (since there's never been a wr).
         * HAVE TO GET SELECTIVE RECEIVE BACK.
*)

protocol Replication concern core {

constant Key
constant NIL // fixme should be in auto gen part of config

roles Client, Replica

interaction basic_rd_wr {
  msg Client  -> Set<Replica> rd (Key k)
  msg Replica -> Client       rdack(Key k, BOOLEAN ok, NIL<BOOLEAN> data)
  msg Client  -> Set<Replica> wr (Key k, BOOLEAN data)
  msg Replica -> Client       wrack(Key k)

  display swimlane {
    msg rdack color = IF ok THEN green ELSE orange
    msg wr    color = blue
    msg wrack color = blue
  }

  tla { rdval(rdack) == any2(rdack,"data") }

  role Client {
    (* InitDownReplica is defaulted to {}, unless listed in the .config file*) 
    state views Set<Replica> = Replica \ InitDownReplica 
    state Key cur_k = CHOOSE k \in Key: TRUE

    handle event rdwr(Key new_k) {
      view(Replica) !! rd(k = new_k)
      change cur_k = new_k (* needed to safe k for use past await *)
      (* !!! the rdack below is not an extend hook! i.e. does not get merged *)
      (* because it's viewed as a PC label *)
      await msg rdack from all(Replica) 
                      where all rdack.ok @rd_pending, @rdack(rdack)
      let newdata = ~rdval(rdack)
      view(Replica) !! wr(k = cur_k, data = newdata) (* flip value *)
      await msg wrack from all(Replica) @wr_pending, @wrack(wrack)
    }
  }

  role Replica() {
    persistent state Map<Key, BOOLEAN> locked
    persistent state Map<Key, BOOLEAN> value

    handle msg rd {
	|  locked[rd.k] -> 
	     reply rdack(k=rd.k, data=NIL, ok=FALSE)
        | ~locked[rd.k] -> @lock(rd.k, rd.sender)
	     reply rdack(k=rd.k, data=value[rd.k], ok=TRUE )
	     change locked[rd.k] = TRUE
    }

    handle msg wr { @unlock(wr.k), @wr(wr)
      reply wrack(k=wr.k)
      change value[wr.k] = wr.data, locked[wr.k] = FALSE
    }
  }
}


interaction deadlock_handling {
  msg Client -> Replica release(Key k)

  display swimlane { 
    msg release color = yellow
  }

  role Client() {
    use msg rdack of rdwr
    use state cur_k of rdwr
    handle msg rdack from all(Replica) where some ~rdack.ok { @rdack(rdack)
      let lock_ok = { m.sender : m \in { m \in all(rdack) : m.ok } }
      (* assume in-order msg delivery, no need to await ack for release *)
      lock_ok !! release(k=any2(rdack, "k"))
      rewind rdwr to init // retry the rd operation
    }
  }

  role Replica() {
    use state locked of rdwr
    handle msg release { @unlock(release.k)
      change locked[release.k] = FALSE
    }
  }
}

interaction check_this {
  tla { RS == Replica }
  tla { Consistency ==
	  \A k \in Key:
	    IF \A r \in RS: ~st_Replica[r].locked[k] (* none locked *)
              THEN \A r,s \in RS:
	             st_Replica[r].value[k] = st_Replica[s].value[k]
              ELSE TRUE
  }

  (* I wish I could use the rdwr@rd_pending syntax *)
  tla { WillCompleteWr == 
          \A c \in Client: 
                (st_Client[c].g_running  /\ st_Client[c].g_pc_rdwr = 1) 
             ~> (~st_Client[c].g_running \/ st_Client[c].g_pc_rdwr = 2)
  }
}

interaction tla_boilerplate {
  tla { SearchView == <<st_Client, st_Replica>> } (* for swimlane bug *)
  tla { Perms == Permutations(Client) \cup Permutations(Replica) }
}

}
